{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VsFsqBkwS2u"
      },
      "source": [
        "# 3rd practice: Neural Networks\n",
        "* **Course**: Artificial Intelligence\n",
        "* **University**: Universitat Rovira i Virgili\n",
        "\n",
        "In this practice students will explore the following fundamentals of machine learning:\n",
        "* Data analysis\n",
        "* Preprocessing\n",
        "* Basic desgin of Multi-Layer Perceptrons (MLP)\n",
        "* Training of neural networks\n",
        "* Results analysis\n",
        "* Decision making / Critical thinking\n",
        "\n",
        "While reading this notebook, please note the following information:\n",
        "* Each section (and subsection) has a title and a brief description of its contents.\n",
        "* If a section (or subsection) title begins with `Student` and/or is **<font color='orange'>written in orange</font>**, the student has work to do inside it, adding code and/or text.\n",
        "* If the title of a section begins with `Teacher`, all its content (including subsections) is part of the work statement. Subsequently, it **MUST NOT** be modified.\n",
        "* Sections must be executed in order.\n",
        "* The addition of imports is not allowed. Everything must be solved with the ones done in `Teacher: Initialization`.\n",
        "* **<font color='orange'>Please enter your full names in the next cell in this section and run it BEFORE going any further.</font>**\n",
        "\n",
        "The practice is designed to be done in **Google Colab**. You can find multiple tutorials on Google Colab and Pandas in Moodle.\n",
        "\n",
        "The dataset to be used (**available on Moodle**) was extracted from [this website](https://www.kaggle.com/datasets/mfarhaannazirkhan/heart-dataset). **The task is to predict the `sex` attribute using all other attributes (except `target`) as inputs.** The `target` attribute cannot be used in any case, neither as input nor as ground truth.\n",
        "\n",
        "\n",
        "## Deliverable\n",
        "It is a practice **in pairs**. The students only need to deliver **this notebook** including the answers. The file must have as name `P3_[NameSurnames1]-[NameSurnames2].ipynb`, replacing `[NameSurnames1]` and `[NameSurnames2]` with those from the students.\n",
        "\n",
        "## Evaluation\n",
        "To be accepted, all practices delivered must contain **the two solutions with different preprocessing steps** and **the solution with a non-basic MLP**.\n",
        "\n",
        "**All text must be written in English.**\n",
        "\n",
        "<font color='red'>**Similar or identical practices will get a grade of 0.**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "isV-YN1wwW3d"
      },
      "outputs": [],
      "source": [
        "STUDENT_NAMES = \"FULL NAMES OF BOTH STUDENTS HERE\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3BrTKOwa8Lb"
      },
      "source": [
        "# Teacher: Initialization\n",
        "Defines the **Imports** and **Device** for the practice. More details in each subsection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2URGi0bbDKP"
      },
      "source": [
        "## Imports\n",
        "The following code cell determines the available packages/libraries.\n",
        "\n",
        "**You CANNOT add any other import, neither here nor anywhere else in the code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qqAqx8e9p_yh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hashlib import sha256\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEcdO0wmbE1-"
      },
      "source": [
        "## Device\n",
        "Checks if there is a GPU for training the neural networks. If not, the CPU will be used. It is recommended to first test if the preprocessing and model definition are correct in a CPU-based environment and then switch to the GPU-based environment for faster training (and prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV40luAAw4hf",
        "outputId": "5dc1abac-aa28-4770-a3b7-335039c11f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is GPU available. Printing GPU information:\n",
            "Wed May 14 11:30:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4080      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
            "|  0%   35C    P8             12W /  320W |     819MiB /  16376MiB |      1%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      3028    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
            "|    0   N/A  N/A      6152    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
            "|    0   N/A  N/A      7144    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
            "|    0   N/A  N/A      7424    C+G   ...on\\136.0.3240.64\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A      9292    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
            "|    0   N/A  N/A      9876    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
            "|    0   N/A  N/A      9884    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     10740    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
            "|    0   N/A  N/A     11804    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
            "|    0   N/A  N/A     13372    C+G   ...995_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
            "|    0   N/A  N/A     15216    C+G   ...995_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
            "|    0   N/A  N/A     15636    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     18116    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE = \"cuda\"\n",
        "  print(\"There is GPU available. Printing GPU information:\")\n",
        "  !nvidia-smi\n",
        "else:\n",
        "  DEVICE = \"cpu\"\n",
        "  print(\"There is no GPU available, using CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkueZrS9aRR"
      },
      "source": [
        "# Teacher: Base code\n",
        "<a name=\"base_code\"></a>\n",
        "\n",
        "Code available for use in the `Student` sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dZQVZVZwRjU"
      },
      "source": [
        "## Data loading\n",
        "The `data_loading` function loads a `.csv` file (such as that available on Moodle) as a Pandas dataframe. **The file must first be loaded into the environment folder** ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACkAAAAqCAYAAAAu9HJYAAABEElEQVRYCWOwsLD4P9gxw2B3IMh9DC8Kuf6D8GB27KgjqRU7oyE5GpLUCgFqmTOaJkdDklohQC1zSEqTVX7q/+/n8YCrUVh1CqOfFXD9nx0h/9+SBg0Woh3pY2/8/3KWAFYH0tqhRDsywMHw/40cPjAGsdGjsilA9f+TAm68noB5BhQboFhBNwMXn2qOBFlAikNBsQKKHVwOQxanqiORDcbFJhQj2PSNOhJbqIyGJDkhMBqS1AoBapkzmrtHQxJbCBDTwIDVzcTQNKkWQQ7H11QjxmEwNTRrYGALXXqJEZ276eUgbPaMOhJbqJAjNhqS5IQaNj2jIYktVMgRGw1JckINm56hEZLYXD7YxIbEPA4AO3Sw4kFc2wQAAAAASUVORK5CYII=). If you use a web browser *other* than Google Chrome, the loading may fail (e.g., the loading circle is red and never ends). To be used in the [Exploratory data analysis](#data_analysis) section.\n",
        "\n",
        "The dataframe to be used in practice is a pseudo-random subset of the full dataset, **different from that of other students**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CdW-8WIeuJYA"
      },
      "outputs": [],
      "source": [
        "LABEL_COL_NAME = \"sex\" # Global constant\n",
        "\n",
        "\n",
        "def data_loading(file_name:str):\n",
        "  original_df = pd.read_csv(file_name)\n",
        "  original_df = original_df.drop(columns=[\"target\"]) # Remove some columns that CANNOT be used # TODO\n",
        "\n",
        "  # Manage the unbalance of the LABEL_COL\n",
        "  original_df = balance_df(original_df, LABEL_COL_NAME, 1.8)\n",
        "\n",
        "  # Renaming of number-coded categories to category name\n",
        "  mapping = {\"sex\": {\"0\":\"Female\", \"1\":\"Male\"},\n",
        "             \"cp\": {\"0\":\"Typical angina\", \"1\":\"Atypical angina\", \"2\":\"Non-anginal pain\", \"3\":\"Asymptomatic\", \"4\":\"Unknown\"},\n",
        "             \"fbs\":{\"0\":\"<=120mg/dl\", \"1\":\">120mg/dl\"},\n",
        "             \"restecg\":{\"0\":\"Normal\", \"1\":\"ST-T wave abnormality\", \"2\":\"Left ventricular hypertrophy\"},\n",
        "             \"exang\":{\"0\":\"Yes\", \"1\":\"No\"},\n",
        "             \"slope\":{\"0\":\"Upsloping\", \"1\":\"Flat\", \"2\":\"Downsloping\", \"3\":\"Unknown\"},\n",
        "             \"thal\":{\"0\":\"Normal\", \"1\":\"Fixed defect\", \"2\":\"Reversible defect\", \"3\":\"Type 3\", \"6\":\"Type 6\", \"7\":\"Type 7\"}}\n",
        "  for col_name, maps in mapping.items():\n",
        "    original_df[col_name] = original_df[col_name].astype(str)\n",
        "    for old_value, new_value in maps.items():\n",
        "        original_df.loc[original_df[col_name] == old_value, col_name] = new_value\n",
        "\n",
        "  # Infer column types\n",
        "  infer_type = lambda x: x if pd.api.types.is_numeric_dtype(x) else pd.Categorical(x)\n",
        "  original_df = original_df.apply(infer_type, axis=0)\n",
        "\n",
        "  # Select subset\n",
        "  frac = 1/3\n",
        "  names_hash = int.from_bytes(bytearray(sha256(STUDENT_NAMES.encode('utf-8')).digest()), byteorder='big')\n",
        "  random_seed = names_hash % (2**32-1) # Seed must be between 0 and 2**32 - 1\n",
        "  student_df = original_df.sample(frac=frac, random_state=random_seed).reset_index()\n",
        "  student_df = student_df.drop(columns=[\"index\"])\n",
        "\n",
        "  return student_df\n",
        "\n",
        "\n",
        "def balance_df(df:pd.DataFrame, label_col_name:str, ratio:float) -> pd.DataFrame:\n",
        "    # Count the occurrences of each value in the column\n",
        "    value_counts = df[label_col_name].value_counts()\n",
        "    \n",
        "    # Identify the majority and minority classes\n",
        "    majority_class = value_counts.idxmax()\n",
        "    minority_class = value_counts.idxmin()\n",
        "    \n",
        "    majority_count = value_counts[majority_class]\n",
        "    minority_count = value_counts[minority_class]\n",
        "\n",
        "    # Determine how many rows to drop\n",
        "    target_majority_count = int(ratio*minority_count)\n",
        "    excess_majority_count = max(0, majority_count - target_majority_count)\n",
        "    \n",
        "    # Retain only the first rows of the majority class\n",
        "    balanced_df = df.copy()\n",
        "    balanced_majority = balanced_df[balanced_df[label_col_name] == majority_class].iloc[:target_majority_count]\n",
        "    balanced_rest = balanced_df[balanced_df[label_col_name] != majority_class]\n",
        "\n",
        "    # Concatenate to get the final balanced DataFrame\n",
        "    balanced_df = pd.concat([balanced_majority, balanced_rest]).reset_index(drop=True)\n",
        "\n",
        "    return balanced_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx3zDPUXU4oA"
      },
      "source": [
        "## Analysis\n",
        "Functions that facilitate the data distribution analysis. The functions receive the Panda's dataframe and the name of the column (or columns) to consider. To be used in the [Exploratory data analysis](#data_analysis) section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vwbgy0_NU7b_"
      },
      "outputs": [],
      "source": [
        "def feature_histogram(df:pd.DataFrame, column_name:str):\n",
        "    column_data = df[column_name]\n",
        "\n",
        "    if pd.api.types.is_numeric_dtype(column_data):\n",
        "        plt.hist(column_data, bins=10)\n",
        "    else:\n",
        "        value_counts = column_data.value_counts()\n",
        "        value_counts.plot(kind='bar')\n",
        "\n",
        "    plt.xlabel(column_name)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of {}'.format(column_name))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def feature_stats(df:pd.DataFrame, column_name:str, verbose:bool=True):\n",
        "  stats = {}\n",
        "  column_data = df[column_name]\n",
        "\n",
        "  # Numerical\n",
        "  if pd.api.types.is_numeric_dtype(column_data):\n",
        "    stats[\"Min\"] = column_data.min()\n",
        "    stats[\"Max\"] = column_data.max()\n",
        "    stats[\"Mean\"] = column_data.mean()\n",
        "    stats[\"Std\"] = column_data.std()\n",
        "\n",
        "  # Categorical\n",
        "  else:\n",
        "    stats[\"Unique\"] = column_data.value_counts()\n",
        "\n",
        "  if verbose and len(stats) > 0:\n",
        "    stats_str = \"\"\n",
        "    for name, value in stats.items():\n",
        "      stats_str += f\" {name}={value} |\"\n",
        "    print(f\"Stats of {column_name}:{stats_str}\")\n",
        "\n",
        "  return stats\n",
        "\n",
        "\n",
        "def features_relationship(df:pd.DataFrame, column_name_1:str, column_name_2:str): \n",
        "    data1 = df[column_name_1]\n",
        "    data2 = df[column_name_2]\n",
        "\n",
        "    # Both columns are numerical\n",
        "    if pd.api.types.is_numeric_dtype(data1) and pd.api.types.is_numeric_dtype(data2):\n",
        "        plt.hexbin(data1, data2, gridsize=30, cmap='Blues', mincnt=1) # Use hexbin for visibility of overlaps\n",
        "        plt.colorbar(label='Count')\n",
        "        plt.xlabel(column_name_1)\n",
        "        plt.ylabel(column_name_2)\n",
        "        plt.title('Relationship between {} and {}'.format(column_name_1, column_name_2))\n",
        "        plt.show()\n",
        "    # Both columns are categorical\n",
        "    elif not pd.api.types.is_numeric_dtype(data1) and not pd.api.types.is_numeric_dtype(data2):\n",
        "        cross_table = pd.crosstab(data1, data2)\n",
        "        cross_table.plot(kind='bar', stacked=True)\n",
        "        plt.xlabel(column_name_1)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Relationship between {} and {}'.format(column_name_1, column_name_2))\n",
        "        plt.show()\n",
        "    # A column is numerical and the other categorical\n",
        "    else:\n",
        "        # Check which is which\n",
        "        if pd.api.types.is_numeric_dtype(data1):\n",
        "            categorical_column = column_name_2\n",
        "            numerical_column = column_name_1\n",
        "        else:\n",
        "            categorical_column = column_name_1\n",
        "            numerical_column = column_name_2\n",
        "        \n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=categorical_column, y=numerical_column, data=df)\n",
        "        plt.title(f'Relationship between {categorical_column} and {numerical_column}')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsojjAzs3dn"
      },
      "source": [
        "## PyTorch dataset\n",
        "Function that transforms the preprocessed Pandas' dataframe into the training and test datasets, both using the PyTorch's TensorDataset class. To be used in the [Experiments](#experiments) section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d33JkXNl79nY"
      },
      "outputs": [],
      "source": [
        "def df_to_dataset(preprocessed_df:pd.DataFrame, label_col_name:str=LABEL_COL_NAME):\n",
        "  if not label_col_name in preprocessed_df.columns:\n",
        "    raise Exception(f\"Dataframe MUST contain the label column [{label_col_name}], preferably in the last column.\")\n",
        "\n",
        "  # Obtain inputs and labels\n",
        "  columns_to_discard = [col_name for col_name in preprocessed_df.columns if col_name == label_col_name]\n",
        "  x = preprocessed_df.drop(columns=columns_to_discard)\n",
        "  y = preprocessed_df[label_col_name]\n",
        "\n",
        "  # Encode labels (string to identifier integer)\n",
        "  label_encoder = LabelEncoder()\n",
        "  y = label_encoder.fit_transform(y)\n",
        "\n",
        "  # Convert data to PyTorch tensors\n",
        "  x_tensor = torch.tensor(x.values, dtype=torch.float)\n",
        "  y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "  # Train-test split: 80% train, 20% test\n",
        "  x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor = train_test_split(x_tensor, y_tensor, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "  # Create PyTorch datasets\n",
        "  train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "  test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "  # Get additional information for the model\n",
        "  input_size = x_train_tensor.shape[1]\n",
        "  num_classes = len(label_encoder.classes_)\n",
        "\n",
        "  return train_dataset, test_dataset, input_size, num_classes, label_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA2hDCcovujy"
      },
      "source": [
        "## Model\n",
        "Basic function for the model **instanciation** and allocation in **DEVICE** (GPU or CPU). To be used in the [Experiments](#experiments) section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h4PJJ85gqhJ5"
      },
      "outputs": [],
      "source": [
        "def create_model(model_class, input_size:int, num_classes:int, verbose:bool=True):\n",
        "  model = model_class(input_size, num_classes)\n",
        "  model.to(DEVICE)\n",
        "\n",
        "  if verbose:\n",
        "    num_parameters = 0\n",
        "    for layer in model.parameters():\n",
        "      num_parameters += layer.nelement() * layer.element_size()\n",
        "    print(f\"The model has {num_parameters} parameters\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzawZ_4kvvl3"
      },
      "source": [
        "## Training\n",
        "Function that trains a **model** with a **dataset**, for a **number of epochs** and using a specific **learning rate (lr)**. To be used in the [Experiments](#experiments) section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z0AXXQK635dI"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module, train_dataset:Dataset, test_dataset:Dataset, num_epochs:int, lr:float):\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Create PyTorch dataloaders with hard-coded batch size\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Set up loss evolution plot\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    plt.ion()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Training and Test Loss Evolution')\n",
        "\n",
        "    train_line, = ax.plot([], [], label='Training Loss', color='blue')\n",
        "    test_line, = ax.plot([], [], label='Test Loss', color='orange')\n",
        "    ax.legend()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            # Move data to GPU\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation phase\n",
        "        mean_train_loss = loss_of_dataloader(model, train_loader, criterion)\n",
        "        train_losses.append(mean_train_loss)\n",
        "        mean_test_loss = loss_of_dataloader(model, test_loader, criterion)\n",
        "        test_losses.append(mean_test_loss)\n",
        "\n",
        "        # Update plot\n",
        "        train_line.set_ydata(train_losses)\n",
        "        train_line.set_xdata(range(1, len(train_losses) + 1))\n",
        "\n",
        "        test_line.set_ydata(test_losses)\n",
        "        test_line.set_xdata(range(1, len(test_losses) + 1))\n",
        "\n",
        "        ax.relim()\n",
        "        ax.autoscale_view()\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        # Print epoch info\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] | Training Loss: {mean_train_loss:.4f} (starting at {train_losses[0]:.4f}) | Test Loss: {mean_test_loss:.4f} (starting at {test_losses[0]:.4f})')\n",
        "\n",
        "    # Turn off interactive mode\n",
        "    plt.ioff()\n",
        "    plt.close()\n",
        "\n",
        "    return train_losses, test_losses\n",
        "\n",
        "def loss_of_dataloader(model:nn.Module, dataloader:DataLoader, criterion):\n",
        "    total_loss = 0.0\n",
        "    num_samples = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item() * len(inputs) # Avg. loss * #samples\n",
        "            num_samples += len(inputs)\n",
        "    model.train()\n",
        "\n",
        "    mean_loss = total_loss / num_samples\n",
        "\n",
        "    return mean_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBcUPiT-vx-g"
      },
      "source": [
        "## Evaluation\n",
        "Functions for the result analysis of the [Experiments](#experiments) section. In particular, for the evaluation of [classification accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy?hl=es-419) and [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yvsz3pZ9qi6V"
      },
      "outputs": [],
      "source": [
        "def full_evaluation(model:nn.Module, train_dataset:Dataset, test_dataset:Dataset, label_encoder:LabelEncoder):\n",
        "    train_accuracy, _, _ = evaluate(model, train_dataset)\n",
        "    test_accuracy, test_all_targets, test_all_predicted = evaluate(model, test_dataset)\n",
        "\n",
        "    print(f'Train accuracy: {train_accuracy*100:.2f}%')\n",
        "    print(f'Test accuracy: {test_accuracy*100:.2f}%')\n",
        "\n",
        "    plot_confusion_matrix(test_all_targets, test_all_predicted, label_encoder)\n",
        "\n",
        "    return train_accuracy, test_accuracy, test_all_targets, test_all_predicted\n",
        "\n",
        "\n",
        "def evaluate(model:nn.Module, dataset:Dataset):\n",
        "  # Create dataloader\n",
        "  eval_loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "  model.eval()  # Set model to evaluation mode\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      all_targets = np.empty(len(dataset), dtype=int)\n",
        "      all_predicted = np.empty_like(all_targets)\n",
        "      for inputs, targets in eval_loader:\n",
        "          # Move data to GPU\n",
        "          inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "          # Store targets and predicted labels\n",
        "          all_targets[total:total+targets.size(0)] = targets.cpu().numpy()\n",
        "          all_predicted[total:total+targets.size(0)] = predicted.cpu().numpy()\n",
        "\n",
        "          # Update total and correct predictions\n",
        "          total += targets.size(0)\n",
        "          correct += (predicted == targets).sum().item()\n",
        "\n",
        "      accuracy = correct / total\n",
        "\n",
        "  return accuracy, all_targets, all_predicted\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(all_targets:np.array, all_predicted:np.array, label_encoder:LabelEncoder):\n",
        "  # Calculate confusion matrix\n",
        "  cm = confusion_matrix(all_targets, all_predicted)\n",
        "\n",
        "  # Plot confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "  plt.xlabel('Predicted labels')\n",
        "  plt.ylabel('True labels')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhUpZgq5fjD3"
      },
      "source": [
        "# <font color='orange'>Student: Exploratory data analysis</font>\n",
        "<a name=\"data_analysis\"></a>\n",
        "\n",
        "<font color='orange'>In this section, the student will have to perform the following tasks:\n",
        "* Load the dataset.\n",
        "* Analyze each feature's data distribution using plots, Pandas code (if needed) and textual reasoning (approximately one paragraph per feature) about them.\n",
        "* Analyze relevant relationships between pairs of features' data distributions with plots, Pandas code (if needed) and textual reasoning (approximately one paragraph per relationship) about them. Focus only on the important pairs to avoid overwhelming combinations.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "64hxfYsTxr0f"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bF5-P_Y0h9t"
      },
      "source": [
        "# Teacher: Baseline solution\n",
        "<a name=\"baseline\"></a>\n",
        "This defines the most basic (and unsuccessful) ready-to-use solution for the task. It includes the main components:\n",
        "* Preprocessing\n",
        "* Model\n",
        "* Training\n",
        "* Evaluation\n",
        "\n",
        "Nonetheless, there are a lot of important things missing, as can be seen from the very poor results. Use this as inspiration for your [Solution 1A](#solution_1a), that **must** be better than this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uStwppMUFXbf"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQhqKKIt0h9t"
      },
      "outputs": [],
      "source": [
        "def preprocessing_0(df):\n",
        "  preprocessed_df = df.copy()\n",
        "  # Define columns to use and categorical\n",
        "  categorical_cols = ['cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
        "  numerical_cols = ['age', 'trestbps', 'chol', 'thalachh', 'oldpeak', 'ca']\n",
        "  columns_to_use = numerical_cols + categorical_cols\n",
        "\n",
        "  # Select columns to use\n",
        "  preprocessed_df = preprocessed_df[columns_to_use]\n",
        "\n",
        "  # Column transformer of the features\n",
        "  column_transformer = ColumnTransformer([('onehot', OneHotEncoder(), categorical_cols)], remainder=\"passthrough\")\n",
        "  preprocessed_df = pd.DataFrame(column_transformer.fit_transform(preprocessed_df))\n",
        "  # IMPORTANT: Students cannot use ColumnTransformer or similars, only Pandas functions\n",
        "\n",
        "  # Label column at the end\n",
        "  preprocessed_df[LABEL_COL_NAME] = df[LABEL_COL_NAME]\n",
        "\n",
        "  return preprocessed_df\n",
        "\n",
        "preprocessed_df = preprocessing_0(df)\n",
        "train_dataset, test_dataset, input_size, num_classes, label_encoder = df_to_dataset(preprocessed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxtTGLqbFZgB"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMfKqYDD0h9t"
      },
      "outputs": [],
      "source": [
        "class Model_0(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Model_0, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, num_classes),\n",
        "            nn.Softmax(dim=1) # Mandatory activation to normalize probabilities between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = create_model(Model_0, input_size, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPFEpDkJFbln"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfENvrBr0h9u"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "losses = train(model, train_dataset, test_dataset, num_epochs=10, lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVEQFf7xGypd"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDHtcS7tGfBf"
      },
      "outputs": [],
      "source": [
        "train_accuracy, test_accuracy, test_all_targets, test_all_predicted = full_evaluation(model, train_dataset, test_dataset, label_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdi-JcS0h9s"
      },
      "source": [
        "# <font color='orange'>Student: Experiments</font>\n",
        "<a name=\"experiments\"></a>\n",
        "\n",
        "<font color='orange'>In this section, the student will have to create two or three consequent solutions for the **Sex classification task**. Each solution must contain the following components:<font color='orange'>\n",
        "* **Idea**: Textual explanation of what is the target/reasoning of the solution (e.g., \"I observed that the previous model had a very reduced size and I want to explore the effects of using a bigger model\").\n",
        "* **Preprocessing**: Prepare the desired features of your dataframe for the neural network. To this end, students can **only use Pandas' functions**.\n",
        "* **Model**: Definition of the Multi-Layer Perceptron model, **only** using [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers and\n",
        " [activations from this list](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n",
        "* **Training**: Perform the learning process trying to maximize the results. This typically implies aiming for convergence.\n",
        "* **Evaluation**: Measuring the performance of the trained model.\n",
        "* **Results analysis**: Textual examination of the solution, focusing in:\n",
        "  * Training\n",
        "  * Accuracy\n",
        "  * Confusion matrix\n",
        "  * Training time\n",
        "</font>\n",
        "\n",
        "<font color='orange'>It can be seen that these are the same as for the baseline solution, but adding the **Idea** and **Results analysis** components.\n",
        "</font>\n",
        "\n",
        "<font color='orange'>**An extended analysis of the baseline results is also requested.**\n",
        "</font>\n",
        "\n",
        "<font color='orange'>None of the solutions has to be the best/ideal, but all of them must be better than the [baseline](#baseline). The idea is to present consequent solutions, being each one the coherent next step of the previous one (the previous step of [Solution 1A](#solution_1a) is the [baseline](#baseline)). This does not imply that each solution has to improve the results of the previous one, but to modify at least one of the components (i.e., preprocessing or model) in a significant and coherent way.\n",
        "</font>\n",
        "\n",
        "<font color='orange'>In particular, we aim for the first two solutions ([Solution 1A](#solution_1a) and [Solution 1B](#solution_1b)) to emphasize preprocessing. Specifically, both will utilize the same basic MLP model (better than that of the [baseline](#baseline)) but experiment with two distinct preprocessing approaches. An explicit comparison of these preprocessing methods and their respective outcomes is necessary. For [Solution 2](#solution_2), the preprocessing method from one of the earlier solutions will be employed. The goal of this final solution is to enhance results by improving/refining the MLP model.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1FFO8IUJJud"
      },
      "source": [
        "## <font color='orange'>Analysis of baseline\n",
        "Provide a comprehensive analysis of the [baseline solution](#baseline). Apart from the results (i.e., **training, accuracy, confusion matrix and runtime**), a review of the solution design (i.e., **preprocessing and model**) is also required. It is not necessary to explain or analysis the code, but the choices. For instace, which features are used and how.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ9k9S4uJMRy"
      },
      "source": [
        "**TODO: YOUR ANALYSIS HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iMQfqcc0h9t"
      },
      "source": [
        "## <font color='orange'>Common functions\n",
        "For avoiding code repetition along all the solutions, you can use this subsection for your common functions and/or classes.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA7dXeUIxyh2"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6e0EGG0h9u"
      },
      "source": [
        "## <font color='orange'>Solution 1A: First preprocessing\n",
        "<a name=\"solution_1a\"></a>\n",
        "\n",
        "Subsequent of [Baseline](#baseline), it employs a basic MLP (better than the provided at the baseline) and a first option for the preprocessing step.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZJDOanXx8YA"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xwZAE4I0h9u"
      },
      "source": [
        "## <font color='orange'>Solution 1B: Second preprocessing solution\n",
        "<a name=\"solution_1b\"></a>\n",
        "\n",
        "Subsequent of the [Solution 1A](#solution_1a), uses the same basic MLP but with an alternative/different preprocessing.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TOhUVc6lx9Kh"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hq-iiM50h9v"
      },
      "source": [
        "## <font color='orange'>Solution 2: New model solution\n",
        "<a name=\"solution_2\"></a>\n",
        "\n",
        "Subsequent of either [Solution 1A](#solution_1a) or [Solution 1B](#solution_1b). Uses the preprocessing step giving the best results previously but now with an improved MLP model.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDpa4tmwx9jI"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "M3BrTKOwa8Lb",
        "X2URGi0bbDKP",
        "QEcdO0wmbE1-",
        "nBkueZrS9aRR",
        "2dZQVZVZwRjU",
        "yx3zDPUXU4oA",
        "MxsojjAzs3dn",
        "gA2hDCcovujy",
        "OzawZ_4kvvl3",
        "jBcUPiT-vx-g",
        "YhUpZgq5fjD3",
        "8bF5-P_Y0h9t",
        "uStwppMUFXbf",
        "nxtTGLqbFZgB",
        "lPFEpDkJFbln",
        "cVEQFf7xGypd",
        "p1FFO8IUJJud",
        "5iMQfqcc0h9t",
        "be6e0EGG0h9u",
        "-xwZAE4I0h9u",
        "3hq-iiM50h9v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "SentTrans",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
